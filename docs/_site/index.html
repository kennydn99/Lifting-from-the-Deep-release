<!DOCTYPE html>
<html lang="en">
  
  <head>
  <meta charset="UTF-8">
  <title>Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/Lifting-from-the-Deep-release/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/Lifting-from-the-Deep-release/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</h1>
  <h2 class="project-tagline">Denis Tome - Chris Russell - Lourdes Agapito</h2>
  <h2 class="project-tagline">Conference on Computer Vision and Pattern Recognition (CVPR) 2017</h2>
  <a href="https://github.com/DenisTome/Lifting-from-the-Deep-release" class="btn">View on GitHub</a>
  <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tome_Lifting_From_the_CVPR_2017_paper.pdf" class="btn">Pdf</a>
  <a href="http://visual.cs.ucl.ac.uk/pubs/liftingFromTheDeep/res/supp_material.pdf" class="btn">Supplementary Material</a>
  <a href="http://visual.cs.ucl.ac.uk/pubs/liftingFromTheDeep/res/slides.pdf" class="btn">Slides</a>
</section>

    <section class="main-content">
      
      <div id="home">
  <div class="posts">
    
      <p><img src="/Lifting-from-the-Deep-release/assets/architecture.jpg" /></p>

<h2 id="abstract">Abstract</h2>

<p>We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB
image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both
tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a
multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine
the search for better 2D locations. The entire process is trained end-to-end, is extremely
efficient and obtains state-of-the-art results on Human3.6M outperforming previous approaches both
on 2D and 3D errors.</p>

<h2 id="examples-on-mpii-dataset">Examples on MPII dataset</h2>

<table cellspacing="0" cellpadding="0">
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/1_042500241.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0001.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/3_003836155.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0003.jpg" class="img_table" /></td>
	</tr>
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/28_027792201.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/img0028.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/32_056733993.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/img0032.jpg" class="img_table" /></td>
	</tr>
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/38_023374079.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0038.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/42_071336755.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/img0042.jpg" class="img_table" /></td>
	</tr>
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/50_083578744.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0050.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/84_086634144.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0084.jpg" class="img_table" /></td>
	</tr>
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/16_013562662.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/img0016.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/34_076858133.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0034.jpg" class="img_table" /></td>
	</tr>
	<tr>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/45_044908796.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/img0045.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/92_093682246.jpg" class="img_table" /></td>
		<td style="width:25%;" align="center"><img src="/Lifting-from-the-Deep-release/assets/MPII/0092.jpg" class="img_table" /></td>
	</tr>
</table>

<h2 id="videos">Videos</h2>

<video controls="" autoplay="" muted="" class="video" width="100%">
	<source src="/Lifting-from-the-Deep-release/assets/media/video_pipeline.mp4" type="video/mp4" />
</video>

<video controls="" autoplay="" muted="" class="video" width="100%">
	<source src="/Lifting-from-the-Deep-release/assets/media/video_results.mp4" type="video/mp4" />
</video>

<h2 id="bibtex">BibTeX</h2>

<figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@InProceedings</span><span class="p">{</span><span class="nl">Tome_2017_CVPR</span><span class="p">,</span>
	<span class="na">author</span> <span class="p">=</span> <span class="s">{Tome, Denis and Russell, Chris and Agapito, Lourdes}</span><span class="p">,</span>
	<span class="na">title</span> <span class="p">=</span> <span class="s">{Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image}</span><span class="p">,</span>
	<span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
	<span class="na">month</span> <span class="p">=</span> <span class="s">{July}</span><span class="p">,</span>
	<span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span>
<span class="p">}</span></code></pre></figure>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>This work has been supported by the <a href="https://secondhands.eu/">SecondHands project</a>, funded from the
EU Horizon 2020 Research and Innovation programme under grant agreement No 643950.</p>

<div style="text-align: center;">
	<div class="aknowledgments_img">
	<img src="/Lifting-from-the-Deep-release/assets/Second-hands-logo_final_sml.jpg" />
	</div>
	<div class="aknowledgments_img">
	<img src="/Lifting-from-the-Deep-release/assets/eu-flag.gif" />
	</div>
</div>

    
  </div>
</div>

      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</a> is maintained by <a href="https://denistome.github.io">Denis Tome</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
